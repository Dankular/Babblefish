# ğŸŸ Babblefish

**Realtime Voice Translation in Your Browser**

*"The Babel Fish is small, yellow, leech-like, and probably the oddest thing in the universe."*
â€” Douglas Adams

---

## Vision

Open a URL on your phone. Select your language. Hear everyone speak to you in your language, in their voice. No app install. No dedicated hardware.

---

## Quick Start

```bash
# Clone
git clone https://github.com/Dankular/Babblefish.git
cd Babblefish

# Setup Python 3.11 environment (required for XTTS-v2)
python setup_tts.py

# Download models (~5GB)
python models/download_server_models.py

# Configure GPU acceleration (optional, requires NVIDIA GPU with 4GB+ VRAM)
cp server/.env.example server/.env
# Edit server/.env:
#   DEVICE=cuda
#   COMPUTE_TYPE=int8

# Run TTS server (standalone XTTS-v2 API)
cd server && python tts_server.py

# Or run full WebSocket server
cd server && python -m uvicorn main:app --host 0.0.0.0 --port 8000

# Run client (separate terminal)
cd client && npm install && npm run dev

# Or use Docker
docker-compose up
```

Open http://localhost:8000/docs for TTS API documentation
Open http://localhost:3000 for the web client

---

## Architecture

```
ğŸ“± Browser Clients (React + WebGPU)
â”œâ”€â”€ Silero VAD (2MB) - Voice activity detection
â”œâ”€â”€ Opus Encoder - Audio compression
â”œâ”€â”€ 3-Tier TTS (adaptive fallback):
â”‚   â”œâ”€â”€ F5-TTS (200MB) - Voice cloning (primary)
â”‚   â”œâ”€â”€ Kokoro-82M (160MB) - Lightweight fallback
â”‚   â””â”€â”€ Server XTTS-v2 API - GPU-accelerated final fallback
â””â”€â”€ IndexedDB - Voice reference storage
        â”‚ WebSocket (opus audio â†‘, json text â†“)
        â–¼
ğŸ–¥ï¸  Server (Python + FastAPI)
â”œâ”€â”€ faster-whisper ASR (~1.5GB, int8) - Speech-to-text (all languages)
â”œâ”€â”€ NLLB-200 Translation (~1.2GB, int8) - 200+ languages
â”œâ”€â”€ XTTS-v2 TTS (~2GB, GPU) - 24 languages with voice cloning
â”œâ”€â”€ Room Manager - WebSocket orchestration
â””â”€â”€ TTS Manager V2 - Complete ASR â†’ Translation â†’ TTS pipeline
```

**Total Server:** ~5GB RAM + 4GB VRAM (with XTTS-v2 GPU acceleration)
**Total Client:** ~360MB download (cached), WebGPU or WASM

---

## NEW: TTS Manager V2 with XTTS-v2

Complete server-side TTS pipeline with voice cloning:

**Features:**
- ğŸ¤ **ASR:** faster-whisper for all languages
- ğŸŒ **Translation:** NLLB-200 (200+ language pairs)
- ğŸ—£ï¸ **TTS:** XTTS-v2 with high-quality voice cloning
- âš¡ **GPU Accelerated:** CUDA support, optimized for 4GB VRAM
- ğŸ­ **Voice Cloning:** Clone any voice from 5-30s reference audio
- ğŸ“¡ **REST API:** Complete FastAPI endpoints

**Quick Test:**
```bash
# Activate Python 3.11 environment
conda activate babblefish-tts  # or: source venv/bin/activate

# Test complete pipeline
python test_full_pipeline_transformers.py

# Verify output
python full_pipeline_verify.py
```

See [TTS_README.md](TTS_README.md) for complete API documentation.

---

## Implementation Status

| Phase | Status | Description |
|-------|--------|-------------|
| **Phase 1** | âœ… Complete | Text translation (ASR + NLLB, 200 languages) |
| **Phase 2** | âœ… Complete | 3-tier TTS architecture (F5 â†’ Kokoro â†’ XTTS-v2) |
| **Phase 3** | âœ… Complete | XTTS-v2 voice cloning + REST API |
| **Phase 4** | ğŸ”„ In Progress | PWA, QR invites, speaker diarization |

---

## Tech Stack

### Server
- **Runtime:** Python 3.11 + FastAPI + uvicorn
- **ASR:** faster-whisper medium (CTranslate2 int8, GPU-accelerated) - ~1.5GB
- **Translation:** NLLB-200-distilled-600M (transformers, GPU-accelerated) - ~1.2GB
- **TTS:** XTTS-v2 (Coqui TTS, GPU-accelerated, voice cloning) - ~2GB
- **GPU:** CUDA support with int8 quantization (4GB VRAM optimal)
- **Protocol:** WebSocket + Opus codec + REST API
- **Storage:** Voice profiles with reference audio

### Client
- **Framework:** React 19 + Vite + Tailwind CSS
- **TTS (3-Tier):** F5-TTS (~200MB) â†’ Kokoro-82M (~160MB) â†’ XTTS-v2 API
- **VAD:** Silero VAD ONNX (~2MB)
- **Inference:** ONNX Runtime Web (WebGPU/WASM)
- **Storage:** IndexedDB (voice references)

---

## Key Features

- **200+ Languages:** NLLB translation coverage
- **24 Language TTS:** XTTS-v2 multilingual synthesis
- **Voice Cloning:** High-quality voice cloning from reference audio
- **Custom Voice Profiles:** Server-side voice profile management
- **3-Tier TTS:** Adaptive fallback (F5-TTS â†’ Kokoro â†’ XTTS-v2)
- **Browser-Based:** No app install, models cached in browser
- **GPU Accelerated:** CUDA support for 2-3x faster processing
- **REST API:** Complete TTS pipeline API with OpenAPI docs
- **Privacy:** Voice references stored securely

---

## TTS API Endpoints

Complete REST API for TTS pipeline (see http://localhost:8000/docs):

```bash
# Synthesize speech with voice cloning
POST /api/tts/synthesize
  - text: Text to synthesize
  - language: Target language (24 languages)
  - voice_profile: Optional voice profile name
  - reference_audio: Optional reference audio file
  - temperature: Voice variation (0.1-1.0)
  - speed: Speech speed (0.5-2.0)

# Transcribe audio (ASR)
POST /api/tts/transcribe
  - audio_file: Audio to transcribe (any language)

# Translate text
POST /api/tts/translate
  - text: Text to translate
  - source_lang: Source language code
  - target_lang: Target language code

# Full pipeline: Audio â†’ Translation â†’ TTS
POST /api/tts/process
  - audio_file: Input audio (any language)
  - target_language: Target language for output
  - voice_profile: Optional voice profile

# Voice profile management
POST /api/tts/voice-profile/add
GET /api/tts/voice-profiles
GET /api/tts/languages
GET /api/tts/status
```

**Example:**
```bash
# Translate English audio to French speech
curl -X POST "http://localhost:8000/api/tts/process" \
  -F "audio_file=@english.wav" \
  -F "target_language=fr" \
  -o french_output.wav
```

---

## Voice Profile API

Create custom voice profiles for XTTS-v2 voice cloning:

```bash
# Add voice profile from audio file
curl -X POST http://localhost:8000/api/tts/voice-profile/add \
  -F "name=charlie" \
  -F "audio_file=@reference.wav" \
  -F "description=Charlie's voice"

# List all voice profiles
curl http://localhost:8000/api/tts/voice-profiles

# Synthesize with voice profile
curl -X POST http://localhost:8000/api/tts/synthesize \
  -F "text=Bonjour le monde" \
  -F "language=fr" \
  -F "voice_profile=charlie" \
  -o output.wav
```

**Best Practices:**
- Use 5-30 seconds of clear speech
- Single speaker, minimal background noise
- WAV or MP3 format, 24kHz recommended
- Longer reference = better voice quality

---

## Supported Languages

### TTS (XTTS-v2) - 24 Languages
English, Spanish, French, German, Italian, Portuguese, Polish, Turkish, Russian, Dutch, Czech, Arabic, Chinese, Japanese, Korean, Hindi, Hungarian, Swedish, Finnish, Danish, Norwegian, Hebrew, Greek, Slovak

### Translation (NLLB-200) - 200+ Languages
All major world languages supported

### ASR (faster-whisper) - 99 Languages
All Whisper-supported languages

---

## WebSocket Protocol

### Client â†’ Server
```typescript
// Join room
{ type: "join", room_id: "abc123", language: "fr", name: "Pierre" }

// Audio chunk (VAD-gated)
{ type: "audio", data: "<base64_opus>", timestamp: 1707400000 }

// Utterance complete
{ type: "utterance_end", timestamp: 1707400002 }

// Leave
{ type: "leave" }
```

### Server â†’ Client
```typescript
// Room joined
{ type: "joined", room_id: "abc123", participant_id: "P_01", participants: [...] }

// Translation result
{
  type: "translation",
  speaker_id: "P_01",
  speaker_name: "Pierre",
  source_lang: "fr",
  source_text: "Bonjour",
  translations: { "en": "Hello", "es": "Hola" },
  timestamp: 1707400002
}

// Participant events
{ type: "participant_joined", participant: {...} }
{ type: "participant_left", participant_id: "P_03" }
```

---

## Models

### Server Models (Required)
```bash
# Automatic download with script
python models/download_server_models.py

# Models downloaded:
# - faster-whisper medium: ~1.5GB (auto-download)
# - NLLB-200: ~1.2GB (requires conversion)
# - XTTS-v2: ~2GB (auto-download on first use)
```

### Client Models (Browser, Auto-Download)
- **F5-TTS:** ~200MB (voice cloning, 3-stage pipeline)
- **Kokoro-82M:** ~160MB (lightweight TTS)
- **Silero VAD:** ~2MB (voice activity detection)

---

## Project Structure

```
Babblefish/
â”œâ”€â”€ server/              # FastAPI async server
â”‚   â”œâ”€â”€ main.py         # WebSocket server entry point
â”‚   â”œâ”€â”€ tts_server.py   # Standalone TTS API server
â”‚   â”œâ”€â”€ config.py       # Configuration
â”‚   â”œâ”€â”€ rooms/          # Room management
â”‚   â”œâ”€â”€ pipeline/       # ASR + Translation
â”‚   â”‚   â”œâ”€â”€ asr.py      # faster-whisper ASR
â”‚   â”‚   â””â”€â”€ translate.py # NLLB translation
â”‚   â”œâ”€â”€ tts/            # TTS engines
â”‚   â”‚   â”œâ”€â”€ xtts_engine.py      # XTTS-v2 engine
â”‚   â”‚   â”œâ”€â”€ tts_manager_v2.py   # Complete pipeline
â”‚   â”‚   â”œâ”€â”€ chatterbox_onnx.py  # Legacy Chatterbox
â”‚   â”‚   â””â”€â”€ voice_profiles.py   # Profile management
â”‚   â”œâ”€â”€ api/            # REST API endpoints
â”‚   â”‚   â””â”€â”€ tts_endpoint.py # TTS API routes
â”‚   â””â”€â”€ transport/      # WebSocket + Opus
â”‚
â”œâ”€â”€ client/             # React browser app
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ core/       # TTS, VAD, audio, voice
â”‚   â”‚   â”œâ”€â”€ network/    # WebSocket, protocol
â”‚   â”‚   â”œâ”€â”€ pages/      # Join, Room
â”‚   â”‚   â”œâ”€â”€ components/ # UI components
â”‚   â”‚   â””â”€â”€ hooks/      # React hooks
â”‚   â””â”€â”€ public/
â”‚
â”œâ”€â”€ models/             # Model download scripts
â”œâ”€â”€ examples/           # Example clients
â”‚   â””â”€â”€ tts_client.py  # Python TTS API client
â”œâ”€â”€ docs/               # Documentation
â”œâ”€â”€ TTS_README.md       # TTS API documentation
â”œâ”€â”€ PYTHON_SETUP.md     # Python 3.11 setup guide
â””â”€â”€ test_*.py          # Pipeline tests
```

---

## Why These Technologies?

**XTTS-v2** over other TTS:
- State-of-the-art voice cloning quality
- 24 language support with consistent quality
- GPU-accelerated inference
- Active development and community

**faster-whisper + NLLB** over SeamlessM4T:
- Better CPU performance (CTranslate2 int8 optimization)
- 200+ languages vs 96
- Independent ASR/translation swapping
- Easier debugging and customization
- Proven production reliability

**Client-Side TTS (F5/Kokoro):**
- No server GPU required for basic operation
- Scales infinitely (compute on each client)
- Lower latency (local synthesis)
- Privacy (models cached locally)
- Fallback to server when needed

---

## GPU Acceleration

BabbleFish supports NVIDIA GPU acceleration for 2-3x faster processing:

**Requirements:**
- NVIDIA GPU with CUDA support (4GB VRAM minimum, 6GB+ recommended)
- CUDA 12.0+ installed
- Python 3.11 (required for XTTS-v2)

**Configuration:**
```bash
# server/.env
DEVICE=cuda
COMPUTE_TYPE=int8  # Optimal for 4GB VRAM

# Or CPU-only mode
DEVICE=cpu
COMPUTE_TYPE=int8
```

**Performance Gains:**
- ASR: 2-3x faster (400ms â†’ 150ms on RTX 3050)
- Translation: 2-3x faster (300ms â†’ 100ms)
- TTS: Real-time factor 0.94x (faster than real-time!)
- Total latency: ~1.5s â†’ ~0.8s end-to-end

**VRAM Usage (int8 quantization):**
- ASR: ~1.2GB
- Translation: ~800MB
- XTTS-v2: ~2GB
- **Total:** ~4GB (optimal for 4-6GB VRAM cards)

**Tested GPUs:**
- âœ… RTX 3050 (4GB) - Works perfectly with int8
- âœ… RTX 3060 (6GB+) - Excellent performance
- âœ… RTX 4060+ - Best performance

**Why int8?**
- 40% less VRAM than float16
- Faster inference than float16 on consumer GPUs
- Minimal quality loss (imperceptible for speech)
- Fits in 4GB VRAM cards

---

## Performance

**End-to-End Latency:**
- CPU-only: ~2-3s after utterance ends
- GPU-accelerated: ~0.8-1.2s after utterance ends

**Pipeline Breakdown (GPU-accelerated):**
- VAD detection: 50-100ms
- Network transfer: 100-200ms
- Server ASR (faster-whisper): 150-250ms
- Server translation (NLLB): 100-200ms
- Server TTS (XTTS-v2): ~1s (real-time factor 0.94x)
- Client TTS: 300-500ms (Kokoro) / 500-800ms (F5-TTS)

**CPU-only (for comparison):**
- Server ASR: 400-600ms
- Server translation: 200-400ms
- Server TTS: ~2-3s

**Test Results (XTTS-v2 Pipeline):**
- Input: 15s English audio
- Output: 6s French audio with cloned voice
- Processing time: ~10s
- GPU: CUDA (RTX series)
- Quality: Excellent voice cloning

---

## Testing & Verification

Complete test suite included:

```bash
# Test XTTS-v2 pipeline (requires Python 3.11)
python test_full_pipeline_transformers.py

# Verify translation accuracy
python full_pipeline_verify.py

# Test with existing Chatterbox (Python 3.13+)
python test_pipeline_existing.py

# Simple XTTS-v2 test
python test_xtts_simple.py

# Test API components
python test_tts_api.py

# Example client usage
python examples/tts_client.py status
python examples/tts_client.py synthesize "Hello world" --lang en
python examples/tts_client.py process audio.wav --to fr
```

**Verification Process:**
1. Transcribe original English audio (ASR)
2. Translate to French (NLLB-200)
3. Synthesize French with voice cloning (XTTS-v2)
4. Transcribe French output to verify accuracy
5. Compare all stages for quality assurance

---

## Development

```bash
# Setup development environment
python setup_tts.py

# Install dependencies
pip install -r server/requirements.txt
cd client && npm install

# Run tests
pytest server/tests/
npm test

# Format code
black server/
prettier --write client/src/

# Lint
pylint server/
npm run lint

# Type check
mypy server/
npm run type-check
```

---

## Reference Implementations

| Component | Repository |
|-----------|------------|
| XTTS-v2 | [coqui-ai/TTS](https://github.com/coqui-ai/TTS) |
| F5-TTS ONNX | [huggingfacess/F5-TTS-ONNX](https://huggingface.co/huggingfacess/F5-TTS-ONNX) |
| F5-TTS Browser | [nsarang/voice-cloning-f5-tts](https://github.com/nsarang/voice-cloning-f5-tts) |
| Kokoro-82M | [hexgrad/Kokoro-82M](https://huggingface.co/hexgrad/Kokoro-82M) |
| faster-whisper | [SYSTRAN/faster-whisper](https://github.com/SYSTRAN/faster-whisper) |
| NLLB-200 | [facebook/nllb-200](https://huggingface.co/facebook/nllb-200-distilled-600M) |
| CTranslate2 | [OpenNMT/CTranslate2](https://github.com/OpenNMT/CTranslate2) |

---

## Documentation

- [TTS API Documentation](TTS_README.md) - Complete TTS Manager V2 API guide
- [Python Setup Guide](PYTHON_SETUP.md) - Python 3.11 environment setup
- [API Docs (Live)](http://localhost:8000/docs) - Interactive OpenAPI documentation

---

## Contributing

See [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

**Current Priorities:**
- PWA support for mobile browsers
- QR code room invites
- Speaker diarization
- Additional language support
- Performance optimizations

---

## License

MIT â€” because language should have no barriers.

---

## Credits

Built with â¤ï¸ using:
- [Coqui TTS](https://github.com/coqui-ai/TTS) (XTTS-v2)
- [faster-whisper](https://github.com/SYSTRAN/faster-whisper) (ASR)
- [NLLB-200](https://ai.meta.com/research/no-language-left-behind/) (Translation)
- [F5-TTS](https://github.com/SWivid/F5-TTS) (Voice Cloning)
- [Kokoro-82M](https://huggingface.co/hexgrad/Kokoro-82M) (Lightweight TTS)

Inspired by Douglas Adams' Babel Fish from *The Hitchhiker's Guide to the Galaxy*.
