# BabbleFish Translation Service

A real-time multi-speaker translation system with client-server architecture:

**Server (CPU-Optimized - 128+ cores):**
- **SeamlessM4T v2** (Meta's all-in-one ASR + Translation)
- **Kokoro TTS** (ONNX Runtime for fast synthesis)
- **Pyannote.audio** (Speaker diarization)

**Thin Clients (Laptop/Phone):**
- Minimal CPU/GPU usage
- Just VAD, mic capture, and audio playback
- WebSocket streaming to server

Optimized for multi-core CPU processing with transformers + ONNX Runtime.

## Features

### Core Translation
- üåç **Translate text** between 200+ languages (NLLB-200)
- üé§ **Transcribe audio** in 90+ languages (Whisper)
- üîÑ **Transcribe + Translate** audio files in one call
- ‚ö° **Optimized inference** with CTranslate2 (3.2x faster)
- üöÄ **FastAPI** REST endpoints
- üíª **CPU & GPU support**

### Real-Time Features
- üéôÔ∏è **WebSocket streaming** - Real-time audio translation
- üîä **Voice Activity Detection** - Smart speech segmentation (Silero VAD)
- üåê **Language auto-detection** - Automatic source language identification
- üéµ **Text-to-Speech** - Kokoro TTS for translated output
- üé≠ **Voice cloning** - Passive F5-TTS speaker training
- üìä **Training progress** - Real-time stats on voice learning
- üë• **Multi-speaker support** - Identify and track multiple speakers (pyannote.audio)
- üé® **Per-speaker voices** - Train unique F5-TTS model for each speaker
- üîç **Speaker diarization** - Automatic speaker identification via voiceprints

## Quick Start (Client-Server Architecture)

### Server Setup (5 minutes)

```bash
# 1. Install dependencies
pip install -r requirements_server.txt

# 2. Authenticate with Hugging Face
huggingface-cli login
# Visit https://huggingface.co/pyannote/embedding and accept terms

# 3. Start server (optimized for 128-core CPU)
python server_cpu.py

# Server ready at: ws://0.0.0.0:9000
```

### Client Setup (30 seconds)

```bash
# 1. Open client_thin.html in browser (laptop or phone)

# 2. Configure:
#    - Server URL: ws://YOUR-SERVER-IP:9000/ws/client
#    - Your name: Dan
#    - Target language: English

# 3. Tap microphone and speak!
```

**That's it!** Server handles all heavy processing. Your laptop/phone just streams audio.

---

## Legacy: Standalone Installation

### 1. Install Dependencies

```bash
pip install -r requirements.txt
```

For model conversion, also install:
```bash
pip install transformers
```

### 2. Setup Models

Run the setup script to download and convert models:

```bash
python setup_models.py
```

**Manual NLLB Conversion** (if needed):

```bash
# Install converter
pip install ctranslate2 transformers sentencepiece

# Convert NLLB model
ct2-transformers-converter \
  --model facebook/nllb-200-distilled-1.3B \
  --output_dir ./models/nllb-ct2 \
  --quantization int8
```

### 3. Update Model Paths

Edit `main.py` and update the model loading section to point to your converted models:

```python
# In load_models() function
ct2_model_path = "./models/nllb-ct2"
```

## Usage

### Start the Server

```bash
python main.py
```

Or with uvicorn:
```bash
uvicorn main:app --host 0.0.0.0 --port 8000 --reload
```

The API will be available at `http://localhost:8000`

Interactive docs: `http://localhost:8000/docs`

### API Endpoints

#### 1. Translate Text

```bash
curl -X POST "http://localhost:8000/translate" \
  -H "Content-Type: application/json" \
  -d '{
    "text": "Hello, how are you?",
    "source_lang": "eng_Latn",
    "target_lang": "spa_Latn"
  }'
```

Response:
```json
{
  "translation": "Hola, ¬øc√≥mo est√°s?",
  "source_lang": "eng_Latn",
  "target_lang": "spa_Latn"
}
```

#### 2. Transcribe Audio

```bash
curl -X POST "http://localhost:8000/transcribe" \
  -F "file=@audio.mp3" \
  -F "language=en"
```

Response:
```json
{
  "text": "This is the transcribed text",
  "language": "en",
  "segments": [
    {"start": 0.0, "end": 2.5, "text": "This is the transcribed text"}
  ]
}
```

#### 3. Transcribe + Translate

```bash
curl -X POST "http://localhost:8000/transcribe-translate" \
  -F "file=@audio.mp3" \
  -F "target_lang=spa_Latn"
```

Response:
```json
{
  "transcription": "Hello, how are you?",
  "detected_language": "en",
  "translation": "Hola, ¬øc√≥mo est√°s?",
  "target_lang": "spa_Latn"
}
```

#### 4. Health Check

```bash
curl http://localhost:8000/health
```

#### 5. Supported Languages

```bash
curl http://localhost:8000/languages
```

## Language Codes

### Whisper (ISO 639-1)
- `en` - English
- `es` - Spanish
- `fr` - French
- `de` - German
- `zh` - Chinese
- [Full list](https://github.com/openai/whisper#available-models-and-languages)

### NLLB-200 (Flores-200)
- `eng_Latn` - English
- `spa_Latn` - Spanish
- `fra_Latn` - French
- `deu_Latn` - German
- `zho_Hans` - Chinese (Simplified)
- `zho_Hant` - Chinese (Traditional)
- `jpn_Jpan` - Japanese
- `kor_Hang` - Korean
- `ara_Arab` - Arabic
- `rus_Cyrl` - Russian
- [Full list](https://github.com/facebookresearch/flores/blob/main/flores200/README.md#languages-in-flores-200)

## Model Sizes

### NLLB-200 Options
- `nllb-200-distilled-600M` - Smallest, fastest
- `nllb-200-distilled-1.3B` - **Recommended** (good balance)
- `nllb-200-3.3B` - Best quality (requires more VRAM)

### Whisper Options
- `tiny` - Fastest, least accurate
- `base` - Good for real-time
- `small` - Balanced
- `medium` - **Recommended** (good quality)
- `large-v2` - Best quality (slower)

## Performance Tuning

### CPU Optimization
```python
# In main.py startup
whisper_model = WhisperModel(
    "medium",
    device="cpu",
    compute_type="int8",  # Quantization
    cpu_threads=4
)
```

### GPU Optimization
```python
whisper_model = WhisperModel(
    "medium",
    device="cuda",
    compute_type="float16"  # Use FP16 on GPU
)

nllb_translator = ctranslate2.Translator(
    model_path,
    device="cuda",
    compute_type="float16"
)
```

### Batch Processing
For multiple translations, use the batch endpoints (to be implemented) for better throughput.

## Docker Deployment (Optional)

```dockerfile
FROM python:3.10-slim

WORKDIR /app

# Install dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application
COPY . .

# Download models (or mount volume)
RUN python setup_models.py

EXPOSE 8000

CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
```

Build and run:
```bash
docker build -t babelfish .
docker run -p 8000:8000 babelfish
```

## ‚≠ê NEW: Client-Server Architecture (Recommended)

### Server-Side CPU Processing (128 cores)

The server does all heavy lifting - optimized for multi-core CPUs:

```bash
# On your powerful server
pip install -r requirements_server.txt

# Authenticate with Hugging Face
huggingface-cli login

# Start CPU server (port 9000)
python server_cpu.py
```

**Server handles:**
- SeamlessM4T v2 (ASR + Translation) - 32 threads
- Kokoro TTS (ONNX) - 16 threads per synthesis
- Speaker diarization - 4 threads
- Multi-client WebSocket - 8 threads

### Thin Client (Laptop/Phone)

Clients use minimal resources - just streaming and playback:

```bash
# On laptop or phone browser
# Open: client_thin.html
# Configure server URL (e.g., ws://192.168.1.100:9000/ws/client)
# Select target language
# Tap microphone and speak!
```

**Client handles:**
- Microphone capture
- Simple VAD (energy-based)
- WebSocket streaming to server
- Audio playback from server

**Benefits:**
- ‚úÖ Laptop GPU completely free (4GB VRAM available for F5-TTS later)
- ‚úÖ Laptop CPU minimal usage (<10%)
- ‚úÖ Server handles multiple clients simultaneously
- ‚úÖ ~1.5-2s latency (acceptable for conversations)
- ‚úÖ Works on phones, tablets, laptops

**See:** Architecture diagrams below

---

## Legacy: Standalone Real-Time Translation

### Quick Start (TTS Version)

```bash
# Install TTS dependencies
pip install -r requirements_tts.txt

# Start TTS server
python realtime_server_tts.py

# Open browser
# Navigate to http://localhost:8002
```

**Features:**
- Microphone capture with VAD
- Real-time transcription & translation
- Kokoro TTS for English output
- Passive speaker voice training (F5-TTS)
- Training progress tracking in UI

**See:** [TTS_SETUP.md](TTS_SETUP.md) for detailed guide

### Server Comparison

#### Port 9000 - CPU Server (Client-Server Architecture) ‚≠ê RECOMMENDED
```bash
python server_cpu.py
```
- **Client-server model** - thin clients connect via WebSocket
- **SeamlessM4T v2** - All-in-one ASR + translation
- **Multi-core optimized** - Uses 60-70 of 128 cores
- **Multi-client support** - Multiple people can connect
- **Speaker diarization** - Identifies speakers across all clients
- **Kokoro TTS** - Fast CPU-based synthesis
- **Latency:** ~1.5-2s per utterance
- **Best for:** Production use with powerful server

#### Port 8000 - REST API Server (Legacy)
```bash
python main.py
```
- REST endpoints for text/audio
- Batch processing
- Good for integrations

#### Port 8001 - Real-Time Server (No TTS)
```bash
python realtime_server.py
```
- WebSocket streaming
- VAD-based segmentation
- Language detection
- Text-only output

#### Port 8002 - Real-Time TTS Server (Single Speaker)
```bash
python realtime_server_tts.py
```
- Everything from 8001 +
- Kokoro TTS output
- Passive speaker training
- F5-TTS voice cloning
- Single speaker mode

#### Port 8003 - Multi-Speaker TTS Server ‚≠ê
```bash
python realtime_server_multispeaker.py
```
- Everything from 8002 +
- **Speaker diarization** (pyannote.audio)
- **Per-speaker voice training** (separate F5-TTS models)
- **Speaker identification** (voiceprint matching)
- **Color-coded UI** (track multiple speakers)
- Best for: meetings, conferences, multi-person conversations

**See:** [MULTISPEAKER_SETUP.md](MULTISPEAKER_SETUP.md) for detailed guide

## Architecture

### Client-Server (Port 9000) ‚≠ê RECOMMENDED

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Thin Clients (Minimal CPU)                   ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ   Dan's Phone    ‚îÇ  Marek's Phone   ‚îÇ    Pierre's Laptop        ‚îÇ
‚îÇ   (English)      ‚îÇ   (Polish)       ‚îÇ    (French)               ‚îÇ
‚îÇ                  ‚îÇ                  ‚îÇ                           ‚îÇ
‚îÇ ‚Ä¢ Mic capture    ‚îÇ ‚Ä¢ Mic capture    ‚îÇ ‚Ä¢ Mic capture             ‚îÇ
‚îÇ ‚Ä¢ Simple VAD     ‚îÇ ‚Ä¢ Simple VAD     ‚îÇ ‚Ä¢ Simple VAD              ‚îÇ
‚îÇ ‚Ä¢ WebSocket      ‚îÇ ‚Ä¢ WebSocket      ‚îÇ ‚Ä¢ WebSocket               ‚îÇ
‚îÇ ‚Ä¢ Playback       ‚îÇ ‚Ä¢ Playback       ‚îÇ ‚Ä¢ Playback                ‚îÇ
‚îÇ                  ‚îÇ                  ‚îÇ                           ‚îÇ
‚îÇ CPU: <10%        ‚îÇ CPU: <10%        ‚îÇ CPU: <10%                 ‚îÇ
‚îÇ GPU: FREE        ‚îÇ GPU: FREE        ‚îÇ GPU: FREE (3050 - 4GB)    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ                  ‚îÇ                  ‚îÇ
         ‚îÇ    WebSocket (audio chunks)         ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ         CPU Server (128 cores, Heavy Lifting)          ‚îÇ
         ‚îÇ                   Port 9000                            ‚îÇ
         ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
         ‚îÇ                                                        ‚îÇ
         ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ
         ‚îÇ  ‚îÇ  Multi-Client Handler (8 threads)            ‚îÇ     ‚îÇ
         ‚îÇ  ‚îÇ  Manages Dan, Marek, Pierre connections      ‚îÇ     ‚îÇ
         ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ
         ‚îÇ      ‚îÇ                                                 ‚îÇ
         ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îÇ
         ‚îÇ  ‚îÇ  Speaker Diarization (4 threads)           ‚îÇ       ‚îÇ
         ‚îÇ  ‚îÇ  Identifies: Pierre, Dan, Marek via        ‚îÇ       ‚îÇ
         ‚îÇ  ‚îÇ  pyannote embeddings (voiceprints)         ‚îÇ       ‚îÇ
         ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îÇ
         ‚îÇ      ‚îÇ                                                 ‚îÇ
         ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îÇ
         ‚îÇ  ‚îÇ  SeamlessM4T v2 Medium (32 threads)        ‚îÇ       ‚îÇ
         ‚îÇ  ‚îÇ  ‚Ä¢ ASR: Transcribe speech                  ‚îÇ       ‚îÇ
         ‚îÇ  ‚îÇ  ‚Ä¢ Translation: To target language         ‚îÇ       ‚îÇ
         ‚îÇ  ‚îÇ  ‚Ä¢ Language detection: Auto-detect source  ‚îÇ       ‚îÇ
         ‚îÇ  ‚îÇ  Pierre (FR) ‚Üí "On devrait..."             ‚îÇ       ‚îÇ
         ‚îÇ  ‚îÇ    ‚Üí EN: "We should..."                    ‚îÇ       ‚îÇ
         ‚îÇ  ‚îÇ    ‚Üí PL: "Powinni≈õmy..."                   ‚îÇ       ‚îÇ
         ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îÇ
         ‚îÇ      ‚îÇ                                                 ‚îÇ
         ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îÇ
         ‚îÇ  ‚îÇ  Kokoro TTS (16 threads √ó N parallel)      ‚îÇ       ‚îÇ
         ‚îÇ  ‚îÇ  ONNX Runtime for fast CPU synthesis       ‚îÇ       ‚îÇ
         ‚îÇ  ‚îÇ  Thread pool A: EN ‚Üí audio (for Dan)       ‚îÇ       ‚îÇ
         ‚îÇ  ‚îÇ  Thread pool B: PL ‚Üí audio (for Marek)     ‚îÇ       ‚îÇ
         ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îÇ
         ‚îÇ      ‚îÇ                                                 ‚îÇ
         ‚îÇ  Resource Usage:                                       ‚îÇ
         ‚îÇ  ‚Ä¢ Cores used: ~60-70 of 128                          ‚îÇ
         ‚îÇ  ‚Ä¢ RAM: ~5GB                                          ‚îÇ
         ‚îÇ  ‚Ä¢ Remaining: 58-68 cores free                        ‚îÇ
         ‚îÇ                                                        ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                ‚îÇ                                       ‚îÇ
                ‚îÇ    WebSocket (synth audio back)      ‚îÇ
                ‚îÇ                                       ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ   Dan's Phone    ‚îÇ        ‚îÇ    Marek's Phone         ‚îÇ
    ‚îÇ   Hears Pierre   ‚îÇ        ‚îÇ    Hears Pierre          ‚îÇ
    ‚îÇ   in English     ‚îÇ        ‚îÇ    in Polish             ‚îÇ
    ‚îÇ   üîä             ‚îÇ        ‚îÇ    üîä                    ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Latency Timeline:
  0ms      - Pierre speaks
  200ms    - VAD detects speech
  1500ms   - VAD detects pause, sends to server
  1600ms   - Speaker ID: "Pierre"
  2400ms   - SeamlessM4T: ASR + Translation
  3000ms   - Kokoro TTS (parallel for EN + PL)
  3000ms   - Audio plays on Dan's and Marek's phones

Total: ~1.5-2s after speech ends
```

### Legacy: REST API (Port 8000)
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   FastAPI App   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ         ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ NLLB  ‚îÇ ‚îÇWhisper ‚îÇ
‚îÇ  CT2  ‚îÇ ‚îÇ  CT2   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚îÇ        ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ GPU/CPU  ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Real-Time with TTS (Port 8002)
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Browser    ‚îÇ
‚îÇ (Microphone) ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ WebSocket
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Silero VAD    ‚îÇ
‚îÇ Audio Buffer  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ    Whisper    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   Training  ‚îÇ
‚îÇ Transcription ‚îÇ     ‚îÇ   Storage   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   NLLB CT2    ‚îÇ
‚îÇ  Translation  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Kokoro TTS   ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ  F5-TTS     ‚îÇ
‚îÇ   Synthesis   ‚îÇ     ‚îÇ  (Trained)  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Browser     ‚îÇ
‚îÇ  (Speakers)   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## Troubleshooting

### Model conversion fails
- Ensure you have `transformers` installed: `pip install transformers`
- Check you have enough disk space (~4GB for NLLB-1.3B)
- Try running the conversion command manually

### Out of memory
- Use smaller models (`600M` for NLLB, `small` for Whisper)
- Reduce `max_batch_size` in translation calls
- Use `int8` quantization for CPU

### Slow performance
- Use GPU if available (`device="cuda"`)
- Enable `vad_filter` in Whisper for faster audio processing
- Pre-convert models to CT2 format (10-30% faster)

## References

- [NLLB-200 Paper](https://arxiv.org/abs/2207.04672)
- [Whisper Paper](https://arxiv.org/abs/2212.04356)
- [CTranslate2 Docs](https://opennmt.net/CTranslate2/)
- [faster-whisper](https://github.com/guillaumekln/faster-whisper)

## License

MIT License - See LICENSE file

## Contributing

Contributions welcome! Please:
1. Fork the repository
2. Create a feature branch
3. Submit a pull request

## TODO

### Completed ‚úÖ
- [x] WebSocket support for real-time transcription
- [x] Language detection with confidence scores
- [x] CTranslate2 optimization (3.2x speedup)
- [x] Text-to-Speech integration (Kokoro)
- [x] Passive speaker voice training (F5-TTS)
- [x] VAD-based speech segmentation (Silero)
- [x] Real-time browser client

### In Progress üöß
- [ ] Complete F5-TTS training pipeline (currently placeholder)
- [ ] GPU optimization for Whisper (improve RTF)
- [ ] Multilingual TTS support (beyond English)

### Planned üìã
- [ ] Implement batch translation endpoint
- [ ] Model caching and warm-up
- [ ] Rate limiting
- [ ] Authentication
- [ ] Prometheus metrics
- [ ] Docker Compose setup
- [ ] Opus audio encoding (bandwidth optimization)
