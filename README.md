# ğŸŸ Babblefish

**A Realtime Voice Translation Tool**

*"The Babel Fish is small, yellow, leech-like, and probably the oddest thing in the universe."*
â€” Douglas Adams, The Hitchhiker's Guide to the Galaxy

---

## Vision

Open a URL on your phone. Select your language. Hear everyone in the room speak to you in your language, in their voice. No app install. No dedicated hardware. Just a browser.

---

## Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     PARTICIPANT PHONES                       â”‚
â”‚                                                             â”‚
â”‚  ğŸ“± Pierre (FR)       ğŸ“± Marek (PL)       ğŸ“± Dan (EN)      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Silero VAD   â”‚    â”‚ Silero VAD   â”‚    â”‚ Silero VAD   â”‚  â”‚
â”‚  â”‚ Opus Encode  â”‚    â”‚ Opus Encode  â”‚    â”‚ Opus Encode  â”‚  â”‚
â”‚  â”‚ F5-TTS(WebGPUâ”‚    â”‚ F5-TTS(WebGPUâ”‚    â”‚ F5-TTS(WebGPUâ”‚  â”‚
â”‚  â”‚ Voice Refs   â”‚    â”‚ Voice Refs   â”‚    â”‚ Voice Refs   â”‚  â”‚
â”‚  â”‚ Cache (A,C)  â”‚    â”‚ Cache (A,C)  â”‚    â”‚ Cache (A,B)  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚         â”‚                   â”‚                   â”‚           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚   WebSocket       â”‚   WebSocket       â”‚   WebSocket
          â”‚   (opus audio â†‘)  â”‚   (opus audio â†‘)  â”‚   (opus audio â†‘)
          â”‚   (json text  â†“)  â”‚   (json text  â†“)  â”‚   (json text  â†“)
          â–¼                   â–¼                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    BABBLEFISH SERVER                         â”‚
â”‚                    (128-core CPU, no GPU)                    â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Room Manager â”‚  â”‚ Speaker      â”‚  â”‚ ASR + Translation â”‚  â”‚
â”‚  â”‚             â”‚  â”‚ Diarization  â”‚  â”‚                   â”‚  â”‚
â”‚  â”‚ WebSocket   â”‚  â”‚ ECAPA-TDNN   â”‚  â”‚ faster-whisper    â”‚  â”‚
â”‚  â”‚ Hub         â”‚  â”‚ (~80MB, CPU) â”‚  â”‚ medium (int8)     â”‚  â”‚
â”‚  â”‚             â”‚  â”‚              â”‚  â”‚ (~1.5GB, CPU)     â”‚  â”‚
â”‚  â”‚ Routing     â”‚  â”‚ Speaker      â”‚  â”‚                   â”‚  â”‚
â”‚  â”‚ Matrix      â”‚  â”‚ Registry     â”‚  â”‚ NLLB-600M         â”‚  â”‚
â”‚  â”‚             â”‚  â”‚              â”‚  â”‚ (~1.2GB, CPU)     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                             â”‚
â”‚  Total: ~40 threads, ~3GB RAM                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Project Structure

```
Babblefish/
â”‚
â”œâ”€â”€ README.md
â”œâ”€â”€ LICENSE                          # MIT
â”œâ”€â”€ docker-compose.yml               # Full stack orchestration
â”œâ”€â”€ .env.example                     # Configuration template
â”‚
â”œâ”€â”€ server/                          # Python async server
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ pyproject.toml
â”‚   â”‚
â”‚   â”œâ”€â”€ main.py                      # FastAPI + WebSocket entry point
â”‚   â”œâ”€â”€ config.py                    # Server configuration / env vars
â”‚   â”‚
â”‚   â”œâ”€â”€ rooms/                       # Room & session management
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ manager.py               # Room lifecycle (create/join/leave)
â”‚   â”‚   â”œâ”€â”€ room.py                  # Room state (participants, routing matrix)
â”‚   â”‚   â””â”€â”€ participant.py           # Participant model (speaker_id, language, ws)
â”‚   â”‚
â”‚   â”œâ”€â”€ pipeline/                    # ML inference pipeline
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ orchestrator.py          # Pipeline coordinator (audio in â†’ text out)
â”‚   â”‚   â”œâ”€â”€ vad.py                   # Server-side Silero VAD (backup/validation)
â”‚   â”‚   â”œâ”€â”€ diarization.py           # SpeechBrain ECAPA-TDNN speaker embeddings
â”‚   â”‚   â”œâ”€â”€ asr.py                   # faster-whisper (CTranslate2, int8)
â”‚   â”‚   â”œâ”€â”€ translate.py             # NLLB-200-distilled-600M (CTranslate2)
â”‚   â”‚   â””â”€â”€ language.py              # Language detection + code mapping
â”‚   â”‚
â”‚   â”œâ”€â”€ speakers/                    # Speaker registry & voice management
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ registry.py              # Speaker tracking across sessions
â”‚   â”‚   â”œâ”€â”€ embeddings.py            # Speaker embedding comparison / matching
â”‚   â”‚   â””â”€â”€ enrolment.py             # Enrolment flow (accumulate clean speech)
â”‚   â”‚
â”‚   â”œâ”€â”€ transport/                   # WebSocket protocol
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ handler.py               # WS connection lifecycle
â”‚   â”‚   â”œâ”€â”€ protocol.py              # Message types & serialisation
â”‚   â”‚   â””â”€â”€ audio_codec.py           # Opus decode on server side
â”‚   â”‚
â”‚   â””â”€â”€ tests/
â”‚       â”œâ”€â”€ test_pipeline.py
â”‚       â”œâ”€â”€ test_rooms.py
â”‚       â”œâ”€â”€ test_diarization.py
â”‚       â””â”€â”€ conftest.py
â”‚
â”œâ”€â”€ client/                          # Browser client (React + Vite)
â”‚   â”œâ”€â”€ Dockerfile                   # Static file server (nginx)
â”‚   â”œâ”€â”€ package.json
â”‚   â”œâ”€â”€ vite.config.js
â”‚   â”œâ”€â”€ tsconfig.json
â”‚   â”œâ”€â”€ index.html
â”‚   â”‚
â”‚   â”œâ”€â”€ public/
â”‚   â”‚   â”œâ”€â”€ favicon.ico
â”‚   â”‚   â””â”€â”€ manifest.json            # PWA manifest
â”‚   â”‚
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ main.jsx                 # App entry point
â”‚   â”‚   â”œâ”€â”€ App.jsx                  # Root component + routing
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ core/                    # ML inference (runs in Web Worker)
â”‚   â”‚   â”‚   â”œâ”€â”€ f5tts/               # F5-TTS ONNX inference
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ model.js         # F5-TTS 3-stage pipeline (encoderâ†’transformerâ†’decoder)
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ processor.js     # Audio preprocessing (mel spectrogram, normalisation)
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ config.js        # Model paths, NFE steps, chunk sizes
â”‚   â”‚   â”‚   â”‚
â”‚   â”‚   â”‚   â”œâ”€â”€ vad/                 # Voice Activity Detection
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ silero.js        # Silero VAD (ONNX/WASM, ~2MB)
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ processor.js     # Utterance boundary detection + buffering
â”‚   â”‚   â”‚   â”‚
â”‚   â”‚   â”‚   â”œâ”€â”€ audio/               # Audio capture & processing
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ capture.js       # Mic access (getUserMedia)
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ opus.js          # Opus encode via MediaRecorder / manual encoder
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ playback.js      # Audio output queue + Web Audio API
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ utils.js         # RMS normalisation, resampling, silence detection
â”‚   â”‚   â”‚   â”‚
â”‚   â”‚   â”‚   â”œâ”€â”€ device.js            # WebGPU capability detection + WASM fallback
â”‚   â”‚   â”‚   â””â”€â”€ worker.js            # Web Worker entry (F5-TTS + VAD inference)
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ engine/                  # Model lifecycle & communication
â”‚   â”‚   â”‚   â”œâ”€â”€ ModelProvider.jsx    # React context for model state
â”‚   â”‚   â”‚   â”œâ”€â”€ adapters.js          # Model adapter registry (F5-TTS, Kokoro fallback)
â”‚   â”‚   â”‚   â””â”€â”€ serialization.js     # Tensor serialisation for Worker â†” Main thread
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ network/                 # Server communication
â”‚   â”‚   â”‚   â”œâ”€â”€ websocket.js         # WebSocket connection manager (reconnect, heartbeat)
â”‚   â”‚   â”‚   â”œâ”€â”€ protocol.js          # Message types matching server protocol
â”‚   â”‚   â”‚   â””â”€â”€ room.js              # Room join/leave/state sync
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ speakers/                # Speaker management (client-side)
â”‚   â”‚   â”‚   â”œâ”€â”€ registry.js          # Local speaker voice reference cache
â”‚   â”‚   â”‚   â”œâ”€â”€ enrolment.js         # Guided enrolment flow (record 15s, upload)
â”‚   â”‚   â”‚   â””â”€â”€ voiceref.js          # Voice reference storage (IndexedDB)
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ pages/                   # UI screens
â”‚   â”‚   â”‚   â”œâ”€â”€ JoinPage.jsx         # Room code entry + language selection
â”‚   â”‚   â”‚   â”œâ”€â”€ EnrolPage.jsx        # "Please speak for 15 seconds" enrolment
â”‚   â”‚   â”‚   â”œâ”€â”€ RoomPage.jsx         # Main translation view (active session)
â”‚   â”‚   â”‚   â””â”€â”€ LoadingPage.jsx      # Model download progress
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ components/              # Reusable UI components
â”‚   â”‚   â”‚   â”œâ”€â”€ LanguageSelector.jsx # Language picker dropdown
â”‚   â”‚   â”‚   â”œâ”€â”€ SpeakerIndicator.jsx # "Pierre is speaking..." visual indicator
â”‚   â”‚   â”‚   â”œâ”€â”€ TranscriptView.jsx   # Live transcript with translations
â”‚   â”‚   â”‚   â”œâ”€â”€ StatusBar.jsx        # Connection status, model status, latency
â”‚   â”‚   â”‚   â”œâ”€â”€ VolumeIndicator.jsx  # Mic input level visualiser
â”‚   â”‚   â”‚   â””â”€â”€ QRInvite.jsx         # QR code to invite others to room
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ hooks/                   # Custom React hooks
â”‚   â”‚   â”‚   â”œâ”€â”€ useAudioCapture.js   # Mic capture + VAD integration
â”‚   â”‚   â”‚   â”œâ”€â”€ useWebSocket.js      # WS connection with auto-reconnect
â”‚   â”‚   â”‚   â”œâ”€â”€ useTTS.js            # F5-TTS synthesis trigger
â”‚   â”‚   â”‚   â”œâ”€â”€ useRoom.js           # Room state management
â”‚   â”‚   â”‚   â””â”€â”€ useSpeakers.js       # Speaker registry + voice refs
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”‚   â”œâ”€â”€ constants.js         # App-wide constants
â”‚   â”‚   â”‚   â””â”€â”€ logger.js            # Structured logging
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ styles/
â”‚   â”‚       â””â”€â”€ globals.css          # Tailwind + custom styles
â”‚   â”‚
â”‚   â””â”€â”€ tests/
â”‚       â”œâ”€â”€ core/
â”‚       â”‚   â””â”€â”€ f5tts.test.js
â”‚       â””â”€â”€ network/
â”‚           â””â”€â”€ websocket.test.js
â”‚
â”œâ”€â”€ models/                          # Model download & caching scripts
â”‚   â”œâ”€â”€ download_server_models.py    # Script to pull faster-whisper + NLLB
â”‚   â””â”€â”€ README.md                    # Model versions, sizes, licenses
â”‚
â””â”€â”€ docs/
    â”œâ”€â”€ ARCHITECTURE.md              # Detailed architecture documentation
    â”œâ”€â”€ PROTOCOL.md                  # WebSocket message protocol specification
    â”œâ”€â”€ MODELS.md                    # Model selection rationale & benchmarks
    â”œâ”€â”€ DEPLOYMENT.md                # Deployment guide (Docker, bare metal)
    â””â”€â”€ CONTRIBUTING.md
```

---

## WebSocket Protocol

### Client â†’ Server Messages

```typescript
// Join a room
{
  type: "join",
  room_id: "abc123",
  language: "fr",         // ISO 639-1
  name: "Pierre"
}

// Audio chunk (during speech, gated by client-side VAD)
{
  type: "audio",
  data: "<base64 opus>",  // Opus-encoded audio chunk (~200ms)
  timestamp: 1707400000   // Client timestamp for latency tracking
}

// Utterance boundary (client VAD detected pause)
{
  type: "utterance_end",
  timestamp: 1707400002
}

// Speaker enrolment audio
{
  type: "enrol",
  audio: "<base64 PCM>",  // 15s of clean speech, 16kHz mono
}

// Leave room
{
  type: "leave"
}
```

### Server â†’ Client Messages

```typescript
// Room joined confirmation
{
  type: "joined",
  room_id: "abc123",
  participant_id: "P_01",
  participants: [
    { id: "P_02", name: "Marek", language: "pl", enrolled: true }
  ]
}

// New speaker enrolled â€” distribute voice reference
{
  type: "speaker_enrolled",
  participant_id: "P_01",
  name: "Pierre",
  language: "fr",
  reference_audio: "<base64 PCM>",   // ~200KB for 15s @ 16kHz
  reference_text: "Bonjour, je m'appelle Pierre..."  // Whisper transcription
}

// Translation result â€” the core message
{
  type: "translation",
  speaker_id: "P_01",               // Who said it
  speaker_name: "Pierre",
  source_lang: "fr",
  source_text: "On devrait vÃ©rifier la proposition",
  translations: {
    "en": "We should check the proposal",
    "pl": "PowinniÅ›my sprawdziÄ‡ propozycjÄ™"
  },
  timestamp: 1707400002
}

// Participant joined/left
{
  type: "participant_joined",
  participant: { id: "P_03", name: "Dan", language: "en" }
}

{
  type: "participant_left",
  participant_id: "P_03"
}
```

---

## Data Flow â€” Single Utterance

```
Timeline (ms)     Phone (Pierre)         Server                  Phone (Marek)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

0                  Pierre starts
                   speaking French

50                 Silero VAD: speech
                   detected (local)
                   Opus encode begins

100-1500           Streaming opus        Receiving audio
                   chunks via WS â†’       chunks, buffering
                                         with speaker ID

1500               VAD: pause detected
                   sends utterance_end â†’

1600                                     ECAPA-TDNN:
                                         match â†’ "P_01" (Pierre)

1800                                     faster-whisper (int8):
                                         "On devrait vÃ©rifier
                                          la proposition"
                                         lang: "fr"

2200                                     NLLB-600M:
                                         â†’ en: "We should check..."
                                         â†’ pl: "PowinniÅ›my..."

2300                                     Push translation        Receives:
                                         to all phones â†’         { speaker: P_01,
                                                                   text: "PowinniÅ›my...",
                                                                   lang: "pl" }

2400                                                             Lookup P_01 voice ref
                                                                 (cached from enrolment)

2500                                                             F5-TTS (WebGPU):
                                                                 Synthesise Polish text
                                                                 in Pierre's voice
                                                                 (~500ms on modern phone)

3000                                                             ğŸ”Š Marek hears Pierre
                                                                    speaking Polish,
                                                                    in Pierre's voice
```

**End-to-end: ~1.5 seconds after utterance ends**

---

## Phase Plan

### Phase 1 â€” MVP: Text Translation Only (No TTS)

**Goal:** Prove the server pipeline works. Phones display translated text.

- [ ] Server: WebSocket room management
- [ ] Server: Opus decode
- [ ] Server: faster-whisper ASR + language detection
- [ ] Server: NLLB translation to all target languages
- [ ] Client: Mic capture + Opus encode + WebSocket stream
- [ ] Client: Silero VAD (client-side, only send speech)
- [ ] Client: Join room, select language, display translated text
- [ ] Docker: Server containerisation

**Deliverable:** Open URL â†’ join room â†’ speak â†’ see translations as text on all phones.

### Phase 2 â€” TTS: Hear Translations (Generic Voice) âœ… COMPLETE

**Goal:** Add audio output using Chatterbox Multilingual (23 languages, no cloning).

- [x] Client: Chatterbox Multilingual ONNX via ONNX Runtime Web + WebGPU/WASM
- [x] Client: Auto-synthesise incoming translations
- [x] Client: Audio playback queue (handle overlapping utterances)
- [x] Client: WebGPU detection + WASM fallback
- [x] Client: Model download progress UI with loading states
- [x] Client: useTTS hook with status tracking
- [x] Client: TTSStatus component for user feedback

**Deliverable:** Open URL â†’ join room â†’ speak â†’ hear translations in a natural voice (23 languages).

### Phase 3 â€” Voice Cloning: Hear Their Voice

**Goal:** Replace Kokoro with F5-TTS. Each person sounds like themselves.

- [ ] Client: F5-TTS ONNX integration (based on nsarang/voice-cloning-f5-tts)
- [ ] Client: Enrolment flow ("Please speak for 15 seconds")
- [ ] Server: Enrolment pipeline (clean speech extraction, transcription)
- [ ] Server: Broadcast voice references to all participants
- [ ] Client: Voice reference cache (IndexedDB)
- [ ] Client: F5-TTS synthesis with speaker-specific voice reference
- [ ] Client: Graceful degradation (F5-TTS â†’ Kokoro â†’ text-only)

**Deliverable:** Open URL â†’ enrol voice â†’ join room â†’ hear everyone in their voice, in your language.

### Phase 4 â€” Production Hardening

**Goal:** Make it robust for real-world use.

- [ ] Server: Speaker diarization (handle turn-taking, overlapping speech)
- [ ] Server: Translation caching (same text + same target = cached)
- [ ] Client: Adaptive model selection (detect phone GPU capability)
- [ ] Client: PWA support (installable, offline model cache)
- [ ] Client: QR code room invite
- [ ] Server: Latency monitoring + metrics
- [ ] Server: Rate limiting + abuse prevention
- [ ] Docs: Deployment guide for self-hosting
- [ ] Tests: End-to-end integration tests

### Phase 5 â€” Future

- [ ] SeamlessStreaming integration (simultaneous translation, lower latency)
- [ ] Chatterbox Multilingual as alternative TTS (if ONNX port becomes available)
- [ ] Multi-room support on single server
- [ ] Recording / transcript export
- [ ] Noise cancellation preprocessing
- [ ] Whisper fine-tuning for specific accents/domains

---

## Tech Stack Summary

### Server

| Component | Technology | Size | Purpose |
|---|---|---|---|
| Runtime | Python 3.11 + FastAPI + uvicorn | â€” | Async WebSocket server |
| ASR | faster-whisper medium (CTranslate2 int8) | ~1.5 GB RAM | Speech-to-text + lang ID |
| Translation | NLLB-200-distilled-600M (CTranslate2) | ~1.2 GB RAM | Text translation (200 langs) |
| Speaker ID | SpeechBrain ECAPA-TDNN | ~80 MB RAM | Speaker embeddings |
| VAD (backup) | Silero VAD | ~2 MB RAM | Server-side validation |
| Audio codec | Opus via opuslib | â€” | Decode incoming audio |
| **Total** | | **~3 GB RAM, ~40 threads** | |

### Client (Browser)

| Component | Technology | Size (download) | Purpose |
|---|---|---|---|
| TTS (Phase 2) âœ… | Chatterbox Multilingual ONNX | ~500 MB (cached) | Natural TTS, 23 languages |
| TTS (Phase 3) | F5-TTS ONNX (fp16) | ~200 MB (cached) | Voice cloning TTS |
| VAD | Silero VAD ONNX | ~2 MB | Speech/silence detection |
| Inference | ONNX Runtime Web | ~5 MB | WebGPU/WASM model execution |
| Audio | Web Audio API + MediaRecorder | â€” | Capture + playback |
| UI | React 19 + Tailwind CSS | â€” | Interface |
| Build | Vite | â€” | Dev server + bundling |

### Infrastructure

| Component | Requirement |
|---|---|
| Server hardware | Any machine with 4+ CPU cores, 8+ GB RAM (128-core is luxury) |
| GPU | Not required anywhere |
| Network | LAN for lowest latency; works over internet too |
| Client devices | Any phone/laptop with Chrome 121+ / Safari 26+ / Firefox 141+ |
| Docker | Optional but recommended for deployment |

---

## Model Decisions & Rationale

### Why faster-whisper + NLLB (cascaded) over SeamlessM4T (unified)?

- **CPU performance:** CTranslate2 models are specifically optimised for CPU inference with int8 quantisation. SeamlessM4T on CPU is significantly slower.
- **Flexibility:** Can swap ASR or translation model independently. Can add language-pair-specific models (Helsinki-NLP Opus-MT) for better quality on specific pairs.
- **Language coverage:** NLLB covers 200 languages vs SeamlessM4T's ~96 for text output.
- **Debuggability:** Can inspect ASR output before translation. Easier to identify where errors occur.

### Why F5-TTS over Chatterbox Multilingual?

- **Browser-proven:** F5-TTS has a working ONNX + WebGPU implementation (nsarang/voice-cloning-f5-tts). Chatterbox has no ONNX port yet.
- **Size:** F5-TTS at ~200MB fp16 is manageable as a browser download. Chatterbox Multilingual at 500M would be ~500MB+.
- **Voice cloning quality:** F5-TTS and Chatterbox are comparable in voice cloning quality. F5-TTS wins on availability.
- **Trade-off acknowledged:** Chatterbox has better multilingual coverage (23 languages natively). F5-TTS multilingual capability is more limited. If/when Chatterbox gets an ONNX port, it becomes the better choice.

### Why Chatterbox Multilingual for Phase 2?

- **Native 23-language support** â€” English, French, Spanish, German, Italian, Portuguese, Polish, Turkish, Russian, Dutch, Czech, Arabic, Chinese, Japanese, Hungarian, Korean, Hindi, Finnish, Vietnamese, Thai, Danish, Swedish, Ukrainian
- **ONNX + WebGPU support** â€” runs in browser with hardware acceleration
- **~500MB model size** â€” reasonable one-time download with browser caching
- **Natural prosody** â€” better quality than Kokoro for multilingual use case
- No voice cloning, but provides high-quality audio output while F5-TTS integration is developed

---

## Key Reference Implementations

| What | Repo | Relevance |
|---|---|---|
| F5-TTS in browser (WebGPU) | [nsarang/voice-cloning-f5-tts](https://github.com/nsarang/voice-cloning-f5-tts) | Core TTS implementation to adapt |
| Chatterbox Multilingual | [onnx-community/chatterbox-multilingual-ONNX](https://huggingface.co/onnx-community/chatterbox-multilingual-ONNX) | Phase 2 TTS (23 languages) âœ… |
| Transformers.js | [huggingface/transformers.js](https://github.com/huggingface/transformers.js) | ONNX Runtime Web + model loading |
| faster-whisper | [SYSTRAN/faster-whisper](https://github.com/SYSTRAN/faster-whisper) | Server-side ASR |
| CTranslate2 | [OpenNMT/CTranslate2](https://github.com/OpenNMT/CTranslate2) | CPU-optimised inference for NLLB |
| Silero VAD | [snakers4/silero-vad](https://github.com/snakers4/silero-vad) | Voice activity detection |
| SpeechBrain | [speechbrain/speechbrain](https://github.com/speechbrain/speechbrain) | Speaker embeddings (ECAPA-TDNN) |

---

## Supported Languages (Server Translation â€” NLLB-200)

200 languages. Notable coverage for the Babel Fish use case:

**European:** English, French, German, Spanish, Portuguese, Italian, Dutch, Polish, Czech, Slovak, Romanian, Hungarian, Greek, Swedish, Norwegian, Danish, Finnish, Bulgarian, Croatian, Serbian, Slovenian, Lithuanian, Latvian, Estonian, Ukrainian, Russian

**Asian:** Mandarin, Cantonese, Japanese, Korean, Hindi, Bengali, Tamil, Thai, Vietnamese, Indonesian, Malay, Tagalog, Burmese, Khmer

**Middle Eastern / African:** Arabic, Hebrew, Turkish, Persian, Swahili, Amharic, Yoruba, Igbo, Hausa, Zulu

**F5-TTS voice output:** English primary, with emerging multilingual support. The cloned voice speaks in the target language with the source speaker's vocal characteristics. Quality varies by language â€” best results for languages with similar phonetic systems.

---

## Getting Started (Development)

```bash
# Clone
git clone https://github.com/Dankular/Babblefish.git
cd Babblefish

# Server
cd server
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
python models/download_server_models.py   # Downloads ~3GB of models
uvicorn main:app --host 0.0.0.0 --port 8000

# Client (separate terminal)
cd client
npm install
npm run dev
# Opens at http://localhost:5173

# Or with Docker
docker-compose up
```

---

## License

MIT â€” because language should have no barriers.
